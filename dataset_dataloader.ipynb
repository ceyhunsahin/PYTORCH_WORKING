{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/26421880 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d556ac8dce74457a9306e30eb8b8bc4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/29515 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6bb5cd09829b43eabfbf3625950d83b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4422102 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1401018061a745d18c292fef381885f2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/5148 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f90c1e3f7c894cddb0a52da0d144896e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Custom Dataset for your files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: kaggle.json: No such file or directory\r\n",
      "kaggle.json\r\n",
      "chmod: /root/.kaggle/kaggle.json: No such file or directory\r\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/ceyhun/.kaggle/kaggle.json'\r\n",
      "Downloading neuralsntua-image-captioning.zip to /Users/ceyhun/PYTORCH\r\n",
      "100%|█████████████████████████████████████▉| 4.08G/4.08G [05:15<00:00, 15.3MB/s]\r\n",
      "100%|██████████████████████████████████████| 4.08G/4.08G [05:15<00:00, 13.9MB/s]\r\n",
      "unzip:  cannot find or open /content/neuralsntua-image-captioning.zip, /content/neuralsntua-image-captioning.zip.zip or /content/neuralsntua-image-captioning.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!ls ~/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d lefterislymp/neuralsntua-image-captioning\n",
    "!unzip /content/neuralsntua-image-captioning.zip"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (4.26.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (4.64.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (22.0)\r\n",
      "Requirement already satisfied: requests in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (2.28.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (2022.10.31)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: filelock in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (3.9.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (0.13.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from transformers) (0.12.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages (from requests->transformers) (1.26.14)\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "!pip install transformers\n",
    "from transformers import AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class KaggleImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, train_captions, root_dir, transform=None, bert_model='distilbert-base-uncased', max_len=512):\n",
    "        self.df = pd.read_csv(train_captions, header=None, sep='|')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.images = self.df.iloc[:,0]\n",
    "        self.captions = self.df.iloc[:,2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        caption = self.captions[idx]\n",
    "        image_id = self.images[idx]\n",
    "        path_to_image = os.path.join(self.root_dir, image_id)\n",
    "        image = Image.open(path_to_image).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tokenized_caption = self.tokenizer(caption,\n",
    "                                           padding='max_length',  # Pad to max_length\n",
    "                                           truncation=True,  # Truncate to max_length\n",
    "                                           max_length=self.max_len,\n",
    "                                           return_tensors='pt')['input_ids']\n",
    "\n",
    "        return image, tokenized_caption"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
    "train_captions = '/content/train_captions.csv'\n",
    "bert_model = 'distilbert-base-uncased'\n",
    "transform = transforms.Compose([transforms.Resize(256),\n",
    "                                transforms.CenterCrop(224),\n",
    "                                transforms.PILToTensor()])\n",
    "train_dataset = KaggleImageCaptioningDataset(train_captions=train_captions,\n",
    "                                             root_dir=root_dir,\n",
    "                                             transform=transform,\n",
    "                                             bert_model=bert_model)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=64,\n",
    "                          num_workers=2,\n",
    "                          shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "for batch_num, (image, caption) in enumerate(train_loader):\n",
    "    if batch_num > 3:\n",
    "        break\n",
    "    print(f'batch number {batch_num} has {image.shape[0]} images and correspondingly {caption.shape[0]} tokenized captions')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "!pip install torchdata\n",
    "import torchdata.datapipes as dp\n",
    "from torch.utils.data.backward_compatibility import worker_init_fn\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "training_csv = '/content/train_captions.csv'\n",
    "train_dp = dp.iter.FileOpener([training_csv])\n",
    "train_dp = train_dp.parse_csv(delimiter='|')\n",
    "train_dp = train_dp.shuffle(buffer_size=2000)\n",
    "train_dp = train_dp.sharding_filter()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 24\u001B[0m\n\u001B[1;32m     16\u001B[0m     tokenized_caption \u001B[38;5;241m=\u001B[39m tokenizer(caption,\n\u001B[1;32m     17\u001B[0m                                   padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# Pad to max_length\u001B[39;00m\n\u001B[1;32m     18\u001B[0m                                   truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# Truncate to max_length\u001B[39;00m\n\u001B[1;32m     19\u001B[0m                                   max_length\u001B[38;5;241m=\u001B[39mmax_len,\n\u001B[1;32m     20\u001B[0m                                   return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     21\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m:image, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m:tokenized_caption}\n\u001B[0;32m---> 24\u001B[0m train_dp \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_dp\u001B[49m\u001B[38;5;241m.\u001B[39mmap(open_image_from_imagepath)\n\u001B[1;32m     25\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mtrain_dp, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, worker_init_fn\u001B[38;5;241m=\u001B[39mworker_init_fn)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_dp' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "max_len = 512\n",
    "root_dir = '/content/flickr30k-images-ecemod/image_dir'\n",
    "\n",
    "def apply_image_transforms(image):\n",
    "\n",
    "    transform = transforms.Compose([transforms.Resize(256),\n",
    "                                    transforms.CenterCrop(224),\n",
    "                                    transforms.PILToTensor()])\n",
    "    return transform(image)\n",
    "\n",
    "def open_image_from_imagepath(row):\n",
    "    image_id, _, caption = row\n",
    "    path_to_image = os.path.join(root_dir, image_id)\n",
    "    image = Image.open(path_to_image).convert('RGB')\n",
    "    image = apply_image_transforms(image)\n",
    "    tokenized_caption = tokenizer(caption,\n",
    "                                  padding='max_length',  # Pad to max_length\n",
    "                                  truncation=True,  # Truncate to max_length\n",
    "                                  max_length=max_len,\n",
    "                                  return_tensors='pt')['input_ids']\n",
    "    return {'image':image, 'caption':tokenized_caption}\n",
    "\n",
    "\n",
    "train_dp = train_dp.map(open_image_from_imagepath)\n",
    "train_loader = DataLoader(dataset=train_dp, shuffle=True, batch_size=32, num_workers=2, worker_init_fn=worker_init_fn)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_num, batch_dict in enumerate(train_loader):\n",
    "        if batch_num > 2:\n",
    "            break\n",
    "\n",
    "        images, captions = batch_dict['image'], batch_dict['caption']\n",
    "        print(f'Batch {batch_num} has {images.shape[0]} images and correspondingly {captions.shape[0]} captions')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)\n",
    "tokenizer('hi how are you')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bert_model = 'distilbert-base-uncased'    # use any model of your choice\n",
    "tokenizer = AutoTokenizer.from_pretrained(bert_model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
