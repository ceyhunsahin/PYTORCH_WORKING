{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "dataset = datasets.ImageFolder(root='~/PYTORCH/data4/', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [],
   "source": [
    "#vgg19(pretrained=True)\n",
    "from torchvision.models import VGG19_Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): ReLU(inplace=True)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.vgg = vgg19(VGG19_Weights.IMAGENET1K_V1)\n",
    "        self.feature_conv = self.vgg.features[:36]\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.classifier = self.vgg.classifier\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad): # # hook for the gradients of the activations\n",
    "        print(grad)\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        h = x.register_hook(self.activations_hook)# register the hook\n",
    "\n",
    "        x = self.max_pool(x)# apply the remaining pooling\n",
    "        x = x.view((1,-1))\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):# method for the gradient extraction\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self,x):# method for the activation extraction\n",
    "        return self.feature_conv(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drawing CAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceyhun/opt/anaconda3/envs/Dataspell_1_env/lib/python3.8/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 1000])"
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the VGG model\n",
    "vgg = VGG()\n",
    "\n",
    "# set the evaluation mode\n",
    "vgg.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img,_ = next(iter(dataloader))\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = vgg(img)\n",
    "pred.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([787])"
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = vgg(img).argmax(dim=1)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "outputs": [],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "pred[:, 787].backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fbe125e89d0>"
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 480x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pull the gradients out of the model\n",
    "gradients = vgg.get_activations_gradient()\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = vgg.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (630, 612, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#heatmap = heatmap.detach().numpy()\n",
    "img = cv2.imread('data4/ben/kanser2.jpeg')\n",
    "print('Original Dimensions : ',img.shape)\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "\n",
    "heatmap = cv2.resize(heatmap,dim)\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.6 + img\n",
    "cv2.imwrite('data4/map.jpg', superimposed_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "outputs": [],
   "source": [
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "dataset = datasets.ImageFolder(root='~/PYTORCH/data4/', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "outputs": [
    {
     "data": {
      "text/plain": "DenseNet(\n  (features): Sequential(\n    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu0): ReLU(inplace=True)\n    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (denseblock1): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition1): _Transition(\n      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock2): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition2): _Transition(\n      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock3): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer13): _DenseLayer(\n        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer14): _DenseLayer(\n        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer15): _DenseLayer(\n        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer16): _DenseLayer(\n        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer17): _DenseLayer(\n        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer18): _DenseLayer(\n        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer19): _DenseLayer(\n        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer20): _DenseLayer(\n        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer21): _DenseLayer(\n        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer22): _DenseLayer(\n        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer23): _DenseLayer(\n        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer24): _DenseLayer(\n        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition3): _Transition(\n      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock4): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer13): _DenseLayer(\n        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer14): _DenseLayer(\n        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer15): _DenseLayer(\n        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer16): _DenseLayer(\n        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n)"
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import DenseNet201_Weights, DenseNet, densenet201, densenet169, AlexNet, alexnet,DenseNet169_Weights\n",
    "DenseNet()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # get the pretrained DenseNet201 network\n",
    "        self.densenet = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.densenet.features[:16]\n",
    "\n",
    "        # add the average global pool\n",
    "        self.global_avg_pool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "\n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.densenet.classifier\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "\n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "\n",
    "        # don't forget the pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view((1,1664))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "outputs": [],
   "source": [
    "# initialize the VGG model\n",
    "dense = DenseNet()\n",
    "\n",
    "# set the evaluation mode\n",
    "dense.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img, _ = next(iter(dataloader)) ####\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = dense(img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.4288e-01, -2.6410e+00,  4.0346e+00,  5.8102e+00,  4.1263e+00,\n          1.0403e+01,  7.8156e+00, -2.2117e+00, -5.3576e-01, -3.2957e+00,\n         -4.7172e+00, -3.9980e+00,  1.4007e+00, -3.1091e+00, -7.6022e-01,\n         -2.0977e+00, -6.7562e+00, -5.1478e+00, -3.4151e+00, -4.3451e+00,\n         -4.5207e+00,  2.9135e-01,  1.4021e+00, -2.8490e+00,  1.7188e-01,\n         -2.7339e+00, -7.7264e-01,  9.2901e-01, -6.9284e-01, -3.2968e+00,\n          5.6001e-01, -6.5737e-01, -1.6367e+00, -1.0698e+00,  4.6322e-02,\n          4.2307e-01,  1.8525e-02,  1.3849e+00, -8.9548e-01, -3.6973e+00,\n         -3.9707e+00, -1.0382e+00, -2.9455e+00, -1.1287e+00, -2.3976e+00,\n         -3.2386e+00, -2.9496e+00, -4.4504e+00, -4.1692e+00, -1.6397e+00,\n         -8.2266e-01,  1.6475e+00, -7.4108e-02,  7.6922e-01, -1.2572e+00,\n         -2.1610e+00, -3.1439e+00, -5.5672e-01, -7.8202e-01,  2.6771e+00,\n          1.5497e+00, -3.0795e+00,  2.7627e-01,  2.7053e+00, -2.4786e-01,\n          2.1400e+00,  1.9565e+00, -4.3414e+00, -2.8934e-01,  6.4441e+00,\n          1.5713e+00,  2.1045e+00,  2.4600e-01,  2.8422e+00, -1.8539e+00,\n         -4.1177e-01, -1.7700e+00, -1.2942e-01,  5.2469e+00,  1.1968e+00,\n         -8.6206e-01, -2.4906e+00,  3.5492e-01,  8.9806e-01, -5.6846e+00,\n          7.6904e-01,  4.4471e-01, -3.7484e+00,  1.2812e+00, -7.2830e-01,\n         -3.0773e+00, -4.1244e+00, -3.9417e+00, -1.5009e+00, -6.6775e+00,\n         -6.4960e+00, -2.2624e+00, -1.5186e+00, -3.2571e+00, -2.6982e+00,\n         -2.4281e+00, -1.2342e+00, -4.5688e+00, -1.3143e+00, -4.5754e+00,\n         -5.8343e+00, -2.1524e+00,  8.0288e+00,  1.5314e+00,  1.1058e+00,\n          6.4834e+00,  1.1892e+01, -5.1201e-01, -7.5454e-01, -1.9263e+00,\n          4.8998e-01,  9.8064e+00,  4.1191e+00,  4.2773e-01,  1.1257e+00,\n          1.2168e+00, -1.3166e+00,  1.0218e+00, -9.1163e-01, -5.1792e+00,\n         -4.3734e+00,  6.8495e+00, -4.5430e+00, -5.0463e+00, -2.5166e+00,\n         -3.5445e+00, -5.1086e+00, -1.5600e+00, -2.5380e+00, -3.9429e+00,\n         -4.7299e+00, -4.1801e+00, -2.4661e+00, -3.4011e+00, -7.4851e+00,\n         -2.6869e-02, -3.8965e+00, -3.1986e+00, -6.1789e+00, -4.6290e+00,\n         -3.4353e+00, -3.2178e+00,  4.7211e-01,  3.0780e+00, -1.9097e+00,\n         -1.7922e+00, -2.3537e+00, -1.2030e+00, -1.9027e+00, -1.7635e+00,\n         -1.5844e+00, -3.0280e-01, -3.5282e+00, -2.9455e+00,  3.7105e-01,\n         -2.3366e-02, -3.8283e+00, -2.2162e+00,  1.7342e+00, -4.7744e+00,\n          1.2036e+00, -3.3045e+00, -2.6420e+00, -3.8449e+00, -9.4258e-01,\n         -2.1358e+00, -5.9031e+00, -4.7026e+00, -1.9340e+00, -1.3607e-01,\n         -9.3466e-01, -5.8196e-01, -9.3244e-01, -1.7252e+00,  3.6158e-01,\n         -1.7233e+00, -1.8679e+00, -4.0965e+00, -3.3837e+00, -1.6857e+00,\n         -2.1143e+00, -2.7240e+00,  1.5320e-01, -3.2307e+00, -1.4605e+00,\n         -2.9248e+00,  2.1745e+00, -6.4606e+00, -3.0492e+00, -5.2205e+00,\n         -3.1221e-01, -3.8707e+00, -3.1015e+00, -4.5716e+00, -4.8327e+00,\n         -4.1170e+00, -2.7441e+00, -3.9152e+00, -3.7246e+00, -6.7922e-01,\n         -3.0494e+00,  9.1724e-01,  7.1784e-01, -4.7488e-01,  1.7955e-02,\n         -3.6998e+00,  1.6848e-01, -4.1058e+00, -5.6243e-01,  1.2116e+00,\n         -2.8275e+00, -3.8747e-01, -4.0977e+00, -3.4285e+00,  2.0385e-01,\n          2.8983e+00, -6.8757e-01, -2.6675e-01, -2.5498e-01, -1.6268e+00,\n         -3.8116e+00, -2.7986e+00, -2.8347e+00, -3.1482e+00, -3.3684e+00,\n         -4.1685e+00, -5.1105e-01, -4.2873e+00, -2.3861e+00,  1.8359e+00,\n         -1.1441e+00, -6.4094e-02, -2.0109e+00, -2.7184e+00, -2.5172e+00,\n         -4.5007e+00, -2.5820e+00,  6.4070e-01,  8.6206e-01, -6.2340e-01,\n         -1.3314e+00, -3.3977e+00, -3.1921e+00, -5.1977e-01, -1.2694e+00,\n         -7.7245e-01, -3.6824e+00,  3.0417e-01, -1.7447e+00, -1.9817e-01,\n         -1.3984e+00, -2.0923e+00, -1.3237e+00, -1.3841e+00, -2.1349e+00,\n          3.4028e+00, -1.4173e+00, -1.3263e+00, -2.4503e+00, -3.7050e+00,\n         -5.5408e-01, -4.3855e+00, -4.4520e+00, -5.9853e+00,  1.4900e+00,\n          5.5686e-01,  7.5948e-01,  5.5474e-01, -3.5650e-01, -3.1621e+00,\n         -3.5959e+00,  9.3313e-02, -2.6987e+00, -2.1884e+00,  1.2104e+00,\n          8.7510e-01,  9.2594e-01,  2.4218e+00,  5.1301e+00, -2.7112e+00,\n          1.7965e+00,  3.7812e+00,  1.1353e+00,  2.1910e+00,  2.1278e+00,\n         -3.9336e-01,  6.9684e+00,  1.4971e+00,  3.3343e+00, -1.7414e+00,\n         -2.0950e+00, -2.8024e+00, -1.9632e+00, -2.8748e+00, -1.5099e+00,\n         -7.4104e-02,  1.3317e+00,  1.4586e+00, -1.6930e+00, -4.8226e-01,\n          1.5557e-01,  2.0725e+00, -6.6463e+00, -7.3844e-01, -3.9859e+00,\n         -2.3786e+00, -1.9579e+00, -3.2142e+00, -1.2175e+00,  1.6109e+00,\n         -1.6936e+00, -2.5023e-01,  1.9520e+00,  2.8960e+00, -2.2722e+00,\n         -3.3260e+00, -3.3712e+00, -1.1211e+00, -1.2867e+00, -1.2047e+00,\n          1.4102e+00,  5.5120e+00,  2.2788e+00, -7.1537e-01, -1.6314e+00,\n         -4.0394e+00, -1.3336e+00, -3.8703e+00, -1.8800e+00, -4.0646e+00,\n         -4.0285e+00, -6.1846e+00, -3.1248e+00, -1.1049e+00, -1.7534e+00,\n         -1.1517e+00, -3.1593e+00, -4.2687e+00, -1.2400e+00, -3.3298e+00,\n         -3.6405e-01, -2.1628e+00, -1.7277e-01, -2.5208e+00, -4.4201e+00,\n         -3.0223e+00, -1.7547e+00, -4.6722e-01, -4.0561e+00, -2.2782e+00,\n         -3.5059e+00, -2.4427e+00, -2.5434e+00, -4.4181e+00, -4.2020e+00,\n         -3.6763e+00, -1.7004e+00, -1.9970e+00, -2.8418e+00, -3.1769e+00,\n          4.3481e+00, -2.8812e+00,  3.3600e+00,  5.5104e-02,  9.5356e-01,\n         -3.8969e+00, -6.2496e-01, -2.1571e+00,  5.8635e-01, -1.1987e+00,\n         -2.1360e+00, -8.2393e-01,  1.1649e+00,  2.0561e-01, -3.0886e+00,\n         -3.5501e+00, -4.3474e+00, -4.6899e+00, -3.4663e+00, -1.0302e+00,\n         -2.3022e+00, -7.2823e-01, -1.6859e+00, -4.6620e+00, -1.1482e+00,\n          2.7125e-01, -1.2924e+00,  3.2062e+00, -1.3226e-01,  9.9160e-01,\n         -3.5134e+00, -2.6296e+00,  8.7732e-02,  9.9284e-01, -1.7033e+00,\n         -4.5980e-01,  5.1761e+00,  9.8227e-01,  2.3876e-01, -1.7728e+00,\n         -8.8844e-01,  1.0034e+00, -3.2367e+00, -8.2115e-01,  4.7448e+00,\n         -3.1384e+00, -3.3244e+00,  1.7884e+00, -7.7009e-01,  3.3258e+00,\n         -2.5429e+00, -9.3809e-01, -9.3613e-01, -8.3585e-01,  3.5260e+00,\n          6.0046e+00,  2.5217e+00,  2.0442e+00,  1.9997e+00, -1.0160e+00,\n         -1.2087e+00,  3.0629e+00,  5.0289e+00,  1.5543e+00,  6.0118e+00,\n          1.1436e+00,  4.0334e-01, -1.7323e+00, -1.3988e+00, -3.1212e+00,\n          2.1204e+00, -2.4763e+00, -3.7161e-01, -1.3006e+00,  3.5386e+00,\n          1.6616e+00,  2.7194e-02,  1.0849e+00,  5.6680e+00,  4.3152e-01,\n         -2.2684e+00,  3.6886e+00,  1.3740e+00,  5.4440e-01, -1.5913e+00,\n         -2.7671e+00,  1.3968e+01,  2.4993e+00, -4.0820e+00, -3.0500e+00,\n          6.1745e+00, -2.1779e+00, -4.5237e-01,  7.9290e+00, -2.7333e+00,\n         -1.8879e+00,  5.2791e+00, -2.4143e+00,  3.9654e+00,  1.6333e+01,\n          2.3812e+00, -1.6990e+00, -1.9914e+00, -8.7469e-01,  2.9397e+00,\n          2.8012e+00,  8.1935e-01,  1.5101e+00,  5.4596e+00, -2.7617e+00,\n          2.9663e+00, -1.4820e-01,  1.5666e+00, -1.8598e+00,  1.3108e+00,\n         -2.9560e+00, -2.5783e+00, -5.0982e+00, -6.6950e-01, -1.3698e+00,\n         -1.0736e+00,  2.7823e+00,  1.3959e+00,  6.3052e+00,  1.3966e+00,\n          5.3384e+00,  1.6515e+00,  2.7523e+00, -9.6901e-01,  6.3715e+00,\n         -3.7731e+00, -3.8492e+00,  1.8670e+00, -1.9608e-01,  2.4439e+00,\n          1.9442e+00,  1.6739e+00,  2.0396e+00, -5.2894e+00,  5.1944e+00,\n          1.1183e+00, -5.2408e-01,  4.1741e+00,  1.3451e+00, -7.0957e+00,\n         -5.0263e+00, -4.8917e+00,  6.3763e+00,  6.6537e-01,  7.4734e+00,\n          3.7522e+00,  5.1481e+00,  5.4042e-01,  1.0460e+00,  2.6684e-01,\n         -7.8196e-01, -1.8095e+00, -3.1066e+00,  9.5755e-01,  5.3636e+00,\n         -2.0626e+00, -3.5112e+00,  1.0549e-01,  5.2150e+00, -6.4570e-01,\n          1.5750e+00,  8.7670e-01, -2.2784e+00, -1.8051e-01, -1.1253e+00,\n         -2.8818e-01, -2.0966e+00, -6.9075e-01, -2.1039e+00,  4.4197e+00,\n         -1.0966e+00,  5.2351e+00,  2.1633e+00,  2.2072e+00,  7.4444e+00,\n          4.0554e+00,  3.2325e+00, -3.0197e+00, -5.2628e+00,  5.6647e+00,\n         -1.1286e+00,  3.0956e+00,  1.1010e+00, -1.4893e+00, -2.6978e+00,\n         -2.2318e+00,  7.9628e+00,  1.0666e+00, -3.2215e+00,  3.5388e+00,\n         -1.6675e+00, -3.2155e+00,  3.0883e+00,  2.3411e+00, -8.3568e-01,\n          1.2040e+00,  1.4780e+00,  6.8942e+00,  9.9502e-01, -6.6757e+00,\n          7.7041e+00,  1.4132e+00,  3.0254e+00, -1.4457e+00,  1.9095e+00,\n         -3.5945e+00,  5.1872e+00,  6.2971e+00, -6.7135e+00,  1.5388e-01,\n         -3.7467e+00,  4.4449e+00, -3.0676e+00,  2.7152e+00,  3.4472e+00,\n         -3.4401e+00,  1.2081e+00,  1.2058e+00,  4.2934e-01,  7.6181e-01,\n         -9.4881e-01,  9.3601e-01, -3.5856e+00,  1.9920e+00,  2.2819e+00,\n         -2.4341e+00,  5.4618e+00,  5.1780e+00, -1.8937e+00,  1.7914e-01,\n          5.4936e+00, -4.1560e-01, -6.9367e-01, -1.5934e+00,  4.3691e+00,\n          1.2821e+00,  6.0301e+00,  7.1624e+00,  1.7979e+00, -1.8148e+00,\n          2.8916e-01,  2.6977e+00, -1.3472e-01, -5.3700e-01, -5.0443e+00,\n          2.9804e+00,  5.7885e-01, -3.5140e+00,  5.4094e+00,  4.0014e+00,\n          3.3655e-01,  1.5400e-02,  1.0315e-01,  5.7886e+00, -2.5609e+00,\n         -3.0454e+00,  4.7566e+00, -2.8893e+00, -2.5591e+00,  7.6338e-01,\n          4.2309e+00, -1.1678e+00, -2.9736e+00,  6.1202e+00,  1.6433e+00,\n          1.9179e+00,  4.1401e+00,  4.4686e+00, -4.0413e+00, -4.4417e-01,\n          1.1993e+01,  8.0792e-01, -7.1774e-01,  6.1999e+00,  4.8063e+00,\n         -4.6399e-01,  2.7030e+00, -3.1632e-01, -4.1276e+00,  2.6275e+00,\n          2.6024e+00, -1.2322e-02,  1.0349e+00,  1.8301e+00, -1.6714e+00,\n         -8.0595e-01, -4.0964e+00,  5.6565e-01, -1.1496e+00,  1.3658e+00,\n         -2.4933e+00,  1.2333e+00, -7.4452e-01, -1.0189e+00, -2.2784e+00,\n         -1.2301e+00, -2.2413e+00, -1.4158e+00,  6.7495e-02, -2.1458e+00,\n          1.7802e-01, -3.3074e-02,  2.8818e+00,  1.9175e+00,  2.0630e+00,\n         -5.1974e+00,  1.1240e+00,  3.5298e+00,  1.3636e+00,  7.4050e+00,\n         -9.8509e-01,  3.6509e-01,  5.1060e+00, -3.2759e+00,  6.5892e+00,\n         -1.6405e+00, -6.3699e-01, -3.2286e+00, -8.1922e-01, -3.0395e+00,\n         -3.1667e+00,  3.2080e-01,  1.2883e+00,  3.3415e+00, -2.7801e+00,\n          8.8593e+00, -6.6961e-01, -3.7736e+00, -4.1294e-01,  8.6248e-03,\n         -1.0756e+00,  2.5836e+00, -3.8978e-01,  3.9726e+00, -3.3111e+00,\n         -1.4560e+00, -3.5598e+00,  2.6001e+00,  3.0726e+00,  3.3223e+00,\n          2.9866e+00,  1.4118e+00,  6.0775e+00, -5.4740e+00,  1.0998e+01,\n          7.6260e+00, -1.1508e+00, -4.2311e+00,  4.1058e-01,  8.6614e-01,\n          4.0993e+00, -3.3503e+00,  4.1065e+00, -2.2211e+00, -7.7014e-01,\n          3.6394e+00, -1.3620e+00, -3.4466e+00, -1.9344e+00,  6.5459e+00,\n         -1.8637e+00, -5.2899e-01, -4.8473e+00, -1.3857e+00, -1.5495e+00,\n         -2.2776e+00,  2.4884e+00, -1.2566e+00, -3.6340e-02, -7.7150e-01,\n         -1.7696e+00,  4.1316e+00, -3.3605e+00,  3.5089e+00,  5.1148e-01,\n          1.7183e+00,  6.6142e+00,  2.2616e+00,  4.7069e+00,  1.5255e+00,\n         -3.8513e+00, -2.5793e+00,  3.2483e+00,  4.1748e+00, -1.2359e+00,\n          1.4434e+00, -8.2042e-01, -3.7442e+00,  1.5012e+00, -3.0551e+00,\n          1.3229e+00,  1.8841e+00, -4.8578e+00,  1.7351e+00, -4.7029e-01,\n          3.0369e+00, -1.8224e+00,  2.8060e+00,  5.4995e+00,  2.2093e+00,\n         -2.7905e-01, -1.4033e+00,  7.0297e+00,  4.2387e+00,  1.3860e+00,\n         -2.4225e+00,  7.6379e-01,  6.9407e+00,  5.8126e+00,  1.2970e+00,\n         -1.4041e+00, -4.7847e-01,  7.2418e-01,  5.0636e+00,  8.0392e-01,\n          1.3459e+00,  8.5427e-01,  1.2314e+01, -4.3296e+00, -1.8062e+00,\n          1.8861e+00, -9.3716e-01,  5.5024e+00, -1.7122e+00, -1.2149e+00,\n          1.8491e+00,  2.5379e+00,  1.6737e+00, -6.9070e-01, -4.8653e+00,\n          5.0614e-01,  1.3230e+00,  8.8095e-01, -3.1494e+00,  8.5273e-02,\n          9.8821e-01, -2.8885e+00, -3.3884e+00,  2.7393e+00,  4.0405e+00,\n         -2.2271e+00,  3.5995e+00, -6.6115e-01,  6.4324e+00, -1.9829e-02,\n         -2.2005e-01,  4.3790e+00, -5.3142e+00,  4.6029e+00, -2.2418e+00,\n          2.1595e+00,  7.5447e-01,  4.7599e+00,  2.7410e+00, -4.5726e+00,\n         -1.3764e+00,  6.8564e+00,  7.4147e+00,  9.2851e+00, -6.3508e-01,\n          4.6164e+00, -4.3402e-01,  3.2685e+00,  4.9280e-01, -1.8742e-01,\n          7.2741e+00,  1.1436e+00,  1.0083e+00, -2.9209e+00,  4.3646e+00,\n         -7.7576e-01,  1.1522e+00, -2.3868e+00,  3.9370e+00,  2.0375e+00,\n          1.1969e+00,  2.8773e+00,  2.8321e+00, -5.6430e+00,  2.2736e-01,\n          2.4451e+00,  3.3904e+00,  1.9522e+00, -4.1161e+00, -6.5612e-02,\n          5.5615e+00, -7.9817e-01,  7.0878e+00,  4.7674e-01,  3.9737e+00,\n         -2.3239e+00,  6.4490e+00,  1.1974e+00,  2.2744e+00, -2.6941e+00,\n         -4.0099e+00,  4.4872e+00, -2.7090e+00,  3.2153e+00, -9.7171e-01,\n          6.7510e-02, -1.7901e+00, -7.5171e-01,  2.0961e+00, -1.3260e+00,\n          1.6261e+00,  3.5292e+00, -2.2554e+00,  1.6909e+00, -1.3817e+00,\n          3.0757e+00,  2.6861e-01, -4.9100e+00,  1.5868e+00,  2.1616e+00,\n          2.0789e+00, -2.4108e+00,  1.6655e+00,  1.4844e+00,  3.9759e+00,\n         -2.2110e+00,  8.3465e+00,  5.8307e+00,  5.8889e+00, -3.4219e-01,\n         -8.9663e-02,  9.1136e-01,  4.9760e-01,  1.2118e-01, -2.2324e+00,\n          4.8348e+00,  3.6918e+00,  6.2548e+00, -3.2737e-01,  4.3252e-01,\n          1.9183e+00, -8.6782e-01,  1.5953e+00,  8.2022e-01, -3.1119e+00,\n          1.1289e+00, -3.2772e+00, -9.6210e-01, -1.3921e+00, -8.7756e-01,\n          1.2611e+00,  1.3919e+00,  1.4197e+00,  7.2854e-01,  4.0298e+00,\n          3.0122e+00,  4.1272e+00,  7.7524e-01,  1.5916e+00,  1.2217e+00,\n          7.2718e-01,  1.5340e-01, -3.4384e-01, -6.9376e-01,  5.8483e-03,\n          1.5315e+00,  1.4182e+00, -4.5570e-01,  3.1952e-01, -1.2782e+00,\n          1.1317e+00,  4.6062e-01,  1.6560e-01,  6.4386e-01, -1.6058e+00,\n          1.8775e+00,  1.3431e+00,  4.8611e-01,  8.2010e-01,  1.4412e+00,\n          9.4292e-01, -2.0181e-01,  5.0265e+00,  1.2154e+00,  3.7349e-01,\n          1.0191e-01,  1.5122e-01,  1.9612e+00, -4.7754e-01, -1.4100e-01,\n          1.6117e+00,  3.2331e+00,  3.0766e+00, -1.9493e+00,  1.3249e-01,\n          2.0720e+00,  3.4500e+00,  1.0896e+00,  3.6225e+00,  1.2140e+01,\n          1.4718e-02,  2.7413e+00,  4.3023e+00,  3.0687e+00,  1.5032e+00,\n         -4.0404e+00,  2.5521e+00, -2.9340e+00,  8.5300e-01,  2.3818e-01,\n         -6.2326e-02, -2.9228e+00,  7.7025e-01, -1.2147e+00, -2.1204e+00,\n          1.4472e+00,  2.6930e+00, -4.7579e+00,  1.7203e+00, -6.5525e+00,\n          6.5310e-01, -2.0389e+00, -1.5146e+00,  3.5993e+00, -4.7431e+00,\n          2.9822e+00, -4.5280e-01,  1.1612e+00,  3.7262e+00,  1.5197e+00,\n          1.6391e+00,  1.3395e+00,  5.5740e+00, -4.1703e+00, -5.4921e-03]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([464])"
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "outputs": [],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "pred[:, 464].backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fbf0ff3f760>"
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 480x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pull the gradients out of the model\n",
    "gradients = dense.get_activations_gradient()\n",
    "\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = dense.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (630, 612, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "heatmap = heatmap.detach().numpy()\n",
    "img = cv2.imread('data4/ben/kanser2.jpeg')\n",
    "print('Original Dimensions : ',img.shape)\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap = cv2.resize(heatmap,dim)\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.6 + img\n",
    "cv2.imwrite('data4/map.jpg', superimposed_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 4., 6.],\n        [1., 2., 3.]])"
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.Tensor([[2,4,6],[1,2,3]]) ;a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 4., 6.],\n        [1., 2., 3.]], requires_grad=True)"
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad_()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(324., grad_fn=<SumBackward0>)"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.pow(3).sum()\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [],
   "source": [
    "b.backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 12.,  48., 108.],\n        [  3.,  12.,  27.]])"
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "        output = input.mm(weight.t())\n",
    "        print('output',output)\n",
    "        print('weight',weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        print('ceyhun')\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        print('input degerler', input)\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        print('beeeeeen')\n",
    "\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "outputs": [],
   "source": [
    "linear = LinearFunction.apply"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True), tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True))\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1013, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1013, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (torch.randn(5,5,dtype=torch.double,requires_grad=True), torch.randn(3,5,dtype=torch.double,requires_grad=True))\n",
    "print(input)\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Thread(Thread-27, initial)>\n",
      "<Thread(Thread-28, initial)>\n",
      "<Thread(Thread-29, initial)>\n",
      "<Thread(Thread-30, initial)>\n",
      "<Thread(Thread-31, initial)>\n",
      "<Thread(Thread-32, initial)>\n",
      "<Thread(Thread-33, initial)>\n",
      "<Thread(Thread-34, initial)>\n",
      "<Thread(Thread-35, initial)>\n",
      "<Thread(Thread-36, initial)>\n"
     ]
    }
   ],
   "source": [
    "# Define a train function to be used in different threads\n",
    "import threading\n",
    "def train_fn():\n",
    "    x = torch.ones(5, 5, requires_grad=True)\n",
    "    # forward\n",
    "    y = (x + 3) * (x + 4) * 0.5\n",
    "    # backward\n",
    "    y.sum().backward()\n",
    "    # potential optimizer update\n",
    "\n",
    "\n",
    "# User write their own threading code to drive the train_fn\n",
    "threads = []\n",
    "for _ in range(10):\n",
    "    p = threading.Thread(target=train_fn, args=())\n",
    "    print(p)\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "\n",
    "for p in threads:\n",
    "    p.join()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Thread(Thread-27, stopped 123145721466880)>,\n <Thread(Thread-28, stopped 123145738256384)>,\n <Thread(Thread-29, stopped 123145755045888)>,\n <Thread(Thread-30, stopped 123145771835392)>,\n <Thread(Thread-31, stopped 123145788624896)>,\n <Thread(Thread-32, stopped 123145805414400)>,\n <Thread(Thread-33, stopped 123145822203904)>,\n <Thread(Thread-34, stopped 123145838993408)>,\n <Thread(Thread-35, stopped 123145855782912)>,\n <Thread(Thread-36, stopped 123145721466880)>]"
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
