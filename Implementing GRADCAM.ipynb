{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision.models import vgg19\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "dataset = datasets.ImageFolder(root='~/PYTORCH/data/', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "#vgg19(pretrained=True)\n",
    "from torchvision.models import VGG19_Weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (17): ReLU(inplace=True)\n    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (24): ReLU(inplace=True)\n    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (26): ReLU(inplace=True)\n    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (31): ReLU(inplace=True)\n    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (33): ReLU(inplace=True)\n    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (35): ReLU(inplace=True)\n    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg19()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "        self.vgg = vgg19(VGG19_Weights.IMAGENET1K_V1)\n",
    "        self.feature_conv = self.vgg.features[:36]\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
    "        self.classifier = self.vgg.classifier\n",
    "        self.gradients = None\n",
    "\n",
    "    def activations_hook(self, grad): # # hook for the gradients of the activations\n",
    "        print(grad)\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        h = x.register_hook(self.activations_hook)# register the hook\n",
    "\n",
    "        x = self.max_pool(x)# apply the remaining pooling\n",
    "        x = x.view((1,-1))\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):# method for the gradient extraction\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self,x):# method for the activation extraction\n",
    "        return self.feature_conv(x)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Drawing CAM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1000])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the VGG model\n",
    "vgg = VGG()\n",
    "\n",
    "# set the evaluation mode\n",
    "vgg.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img,_ = next(iter(dataloader))\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = vgg(img)\n",
    "pred.size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([386])"
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = vgg(img).argmax(dim=1)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0041,  0.0000,  0.0012,  ...,  0.0000,  0.0002,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0005,  0.0000, -0.0026,  ...,  0.0000, -0.0016,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0027,  0.0000,  0.0013,  ...,  0.0000,  0.0009,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0038,  0.0000, -0.0011,  ...,  0.0000,  0.0037,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0047,  0.0000, -0.0042,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0071,  0.0000, -0.0110,  ...,  0.0000, -0.0035,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0009,  0.0000,  0.0002,  ...,  0.0000, -0.0036,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0003,  0.0000, -0.0009,  ...,  0.0000, -0.0041,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0026,  0.0000,  0.0006,  ...,  0.0000,  0.0034,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0138,  0.0000,  0.0073,  ...,  0.0000,  0.0047,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0032,  0.0000, -0.0099,  ..., -0.0082, -0.0107,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0067,  0.0000, -0.0123,  ...,  0.0000, -0.0093,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0057,  0.0000, -0.0046,  ...,  0.0000, -0.0060,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0067,  0.0000, -0.0092,  ...,  0.0000, -0.0007,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0082,  0.0000, -0.0084,  ...,  0.0000, -0.0066,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0019,  0.0000, -0.0086,  ...,  0.0000, -0.0034,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [-0.0054,  0.0000, -0.0001,  ...,  0.0000,  0.0007,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0044,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0142,  0.0000,  0.0000,  ..., -0.0015,  0.0055,  0.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "pred[:, 386].backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 14, 14])\n",
      "torch.Size([512])\n",
      "torch.Size([1, 512, 14, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7fafa54dd820>"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 480x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pull the gradients out of the model\n",
    "gradients = vgg.get_activations_gradient()\n",
    "print(gradients.shape)\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "print(pooled_gradients.shape)\n",
    "# get the activations of the last convolutional layer\n",
    "activations = vgg.get_activations(img).detach()\n",
    "print(activations.shape)\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (280, 419, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "#heatmap = heatmap.detach().numpy()\n",
    "img = cv2.imread('data/Elephant/3.jpg')\n",
    "print('Original Dimensions : ',img.shape)\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(419, 280)\n"
     ]
    }
   ],
   "source": [
    "print(dim)\n",
    "heatmap= heatmap.detach().numpy()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "heatmap = cv2.resize(heatmap,dim,interpolation = cv2.INTER_AREA)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.6 + img\n",
    "cv2.imwrite('data/map.jpg', superimposed_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "outputs": [],
   "source": [
    "# use the ImageNet transformation\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# define a 1 image dataset\n",
    "dataset = datasets.ImageFolder(root='~/PYTORCH/data4/', transform=transform)\n",
    "\n",
    "# define the dataloader to load that single image\n",
    "dataloader = data.DataLoader(dataset=dataset, shuffle=False, batch_size=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "data": {
      "text/plain": "DenseNet(\n  (features): Sequential(\n    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu0): ReLU(inplace=True)\n    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (denseblock1): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition1): _Transition(\n      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock2): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition2): _Transition(\n      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock3): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer13): _DenseLayer(\n        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer14): _DenseLayer(\n        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer15): _DenseLayer(\n        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer16): _DenseLayer(\n        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer17): _DenseLayer(\n        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer18): _DenseLayer(\n        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer19): _DenseLayer(\n        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer20): _DenseLayer(\n        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer21): _DenseLayer(\n        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer22): _DenseLayer(\n        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer23): _DenseLayer(\n        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer24): _DenseLayer(\n        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (transition3): _Transition(\n      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (denseblock4): _DenseBlock(\n      (denselayer1): _DenseLayer(\n        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer2): _DenseLayer(\n        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer3): _DenseLayer(\n        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer4): _DenseLayer(\n        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer5): _DenseLayer(\n        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer6): _DenseLayer(\n        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer7): _DenseLayer(\n        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer8): _DenseLayer(\n        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer9): _DenseLayer(\n        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer10): _DenseLayer(\n        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer11): _DenseLayer(\n        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer12): _DenseLayer(\n        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer13): _DenseLayer(\n        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer14): _DenseLayer(\n        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer15): _DenseLayer(\n        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n      (denselayer16): _DenseLayer(\n        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      )\n    )\n    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (classifier): Linear(in_features=1024, out_features=1000, bias=True)\n)"
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import DenseNet201_Weights, DenseNet, densenet201, densenet169, AlexNet, alexnet,DenseNet169_Weights\n",
    "DenseNet()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # get the pretrained DenseNet201 network\n",
    "        self.densenet = densenet169(weights=DenseNet169_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # disect the network to access its last convolutional layer\n",
    "        self.features_conv = self.densenet.features[:16]\n",
    "\n",
    "        # add the average global pool\n",
    "        self.global_avg_pool = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "\n",
    "        # get the classifier of the vgg19\n",
    "        self.classifier = self.densenet.classifier\n",
    "\n",
    "        # placeholder for the gradients\n",
    "        self.gradients = None\n",
    "\n",
    "    # hook for the gradients of the activations\n",
    "    def activations_hook(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_conv(x)\n",
    "\n",
    "        # register the hook\n",
    "        h = x.register_hook(self.activations_hook)\n",
    "\n",
    "        # don't forget the pooling\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view((1,1664))\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def get_activations_gradient(self):\n",
    "        return self.gradients\n",
    "\n",
    "    def get_activations(self, x):\n",
    "        return self.features_conv(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "# initialize the VGG model\n",
    "dense = DenseNet()\n",
    "\n",
    "# set the evaluation mode\n",
    "dense.eval()\n",
    "\n",
    "# get the image from the dataloader\n",
    "img, _ = next(iter(dataloader)) ####\n",
    "\n",
    "# get the most likely prediction of the model\n",
    "pred = dense(img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.4288e-01, -2.6410e+00,  4.0346e+00,  5.8102e+00,  4.1263e+00,\n          1.0403e+01,  7.8156e+00, -2.2117e+00, -5.3576e-01, -3.2957e+00,\n         -4.7172e+00, -3.9980e+00,  1.4007e+00, -3.1091e+00, -7.6022e-01,\n         -2.0977e+00, -6.7562e+00, -5.1478e+00, -3.4151e+00, -4.3451e+00,\n         -4.5207e+00,  2.9135e-01,  1.4021e+00, -2.8490e+00,  1.7188e-01,\n         -2.7339e+00, -7.7264e-01,  9.2901e-01, -6.9284e-01, -3.2968e+00,\n          5.6001e-01, -6.5737e-01, -1.6367e+00, -1.0698e+00,  4.6322e-02,\n          4.2307e-01,  1.8525e-02,  1.3849e+00, -8.9548e-01, -3.6973e+00,\n         -3.9707e+00, -1.0382e+00, -2.9455e+00, -1.1287e+00, -2.3976e+00,\n         -3.2386e+00, -2.9496e+00, -4.4504e+00, -4.1692e+00, -1.6397e+00,\n         -8.2266e-01,  1.6475e+00, -7.4108e-02,  7.6922e-01, -1.2572e+00,\n         -2.1610e+00, -3.1439e+00, -5.5672e-01, -7.8202e-01,  2.6771e+00,\n          1.5497e+00, -3.0795e+00,  2.7627e-01,  2.7053e+00, -2.4786e-01,\n          2.1400e+00,  1.9565e+00, -4.3414e+00, -2.8934e-01,  6.4441e+00,\n          1.5713e+00,  2.1045e+00,  2.4600e-01,  2.8422e+00, -1.8539e+00,\n         -4.1177e-01, -1.7700e+00, -1.2942e-01,  5.2469e+00,  1.1968e+00,\n         -8.6206e-01, -2.4906e+00,  3.5492e-01,  8.9806e-01, -5.6846e+00,\n          7.6904e-01,  4.4471e-01, -3.7484e+00,  1.2812e+00, -7.2830e-01,\n         -3.0773e+00, -4.1244e+00, -3.9417e+00, -1.5009e+00, -6.6775e+00,\n         -6.4960e+00, -2.2624e+00, -1.5186e+00, -3.2571e+00, -2.6982e+00,\n         -2.4281e+00, -1.2342e+00, -4.5688e+00, -1.3143e+00, -4.5754e+00,\n         -5.8343e+00, -2.1524e+00,  8.0288e+00,  1.5314e+00,  1.1058e+00,\n          6.4834e+00,  1.1892e+01, -5.1201e-01, -7.5454e-01, -1.9263e+00,\n          4.8998e-01,  9.8064e+00,  4.1191e+00,  4.2773e-01,  1.1257e+00,\n          1.2168e+00, -1.3166e+00,  1.0218e+00, -9.1163e-01, -5.1792e+00,\n         -4.3734e+00,  6.8495e+00, -4.5430e+00, -5.0463e+00, -2.5166e+00,\n         -3.5445e+00, -5.1086e+00, -1.5600e+00, -2.5380e+00, -3.9429e+00,\n         -4.7299e+00, -4.1801e+00, -2.4661e+00, -3.4011e+00, -7.4851e+00,\n         -2.6869e-02, -3.8965e+00, -3.1986e+00, -6.1789e+00, -4.6290e+00,\n         -3.4353e+00, -3.2178e+00,  4.7211e-01,  3.0780e+00, -1.9097e+00,\n         -1.7922e+00, -2.3537e+00, -1.2030e+00, -1.9027e+00, -1.7635e+00,\n         -1.5844e+00, -3.0280e-01, -3.5282e+00, -2.9455e+00,  3.7105e-01,\n         -2.3366e-02, -3.8283e+00, -2.2162e+00,  1.7342e+00, -4.7744e+00,\n          1.2036e+00, -3.3045e+00, -2.6420e+00, -3.8449e+00, -9.4258e-01,\n         -2.1358e+00, -5.9031e+00, -4.7026e+00, -1.9340e+00, -1.3607e-01,\n         -9.3466e-01, -5.8196e-01, -9.3244e-01, -1.7252e+00,  3.6158e-01,\n         -1.7233e+00, -1.8679e+00, -4.0965e+00, -3.3837e+00, -1.6857e+00,\n         -2.1143e+00, -2.7240e+00,  1.5320e-01, -3.2307e+00, -1.4605e+00,\n         -2.9248e+00,  2.1745e+00, -6.4606e+00, -3.0492e+00, -5.2205e+00,\n         -3.1221e-01, -3.8707e+00, -3.1015e+00, -4.5716e+00, -4.8327e+00,\n         -4.1170e+00, -2.7441e+00, -3.9152e+00, -3.7246e+00, -6.7922e-01,\n         -3.0494e+00,  9.1724e-01,  7.1784e-01, -4.7488e-01,  1.7955e-02,\n         -3.6998e+00,  1.6848e-01, -4.1058e+00, -5.6243e-01,  1.2116e+00,\n         -2.8275e+00, -3.8747e-01, -4.0977e+00, -3.4285e+00,  2.0385e-01,\n          2.8983e+00, -6.8757e-01, -2.6675e-01, -2.5498e-01, -1.6268e+00,\n         -3.8116e+00, -2.7986e+00, -2.8347e+00, -3.1482e+00, -3.3684e+00,\n         -4.1685e+00, -5.1105e-01, -4.2873e+00, -2.3861e+00,  1.8359e+00,\n         -1.1441e+00, -6.4094e-02, -2.0109e+00, -2.7184e+00, -2.5172e+00,\n         -4.5007e+00, -2.5820e+00,  6.4070e-01,  8.6206e-01, -6.2340e-01,\n         -1.3314e+00, -3.3977e+00, -3.1921e+00, -5.1977e-01, -1.2694e+00,\n         -7.7245e-01, -3.6824e+00,  3.0417e-01, -1.7447e+00, -1.9817e-01,\n         -1.3984e+00, -2.0923e+00, -1.3237e+00, -1.3841e+00, -2.1349e+00,\n          3.4028e+00, -1.4173e+00, -1.3263e+00, -2.4503e+00, -3.7050e+00,\n         -5.5408e-01, -4.3855e+00, -4.4520e+00, -5.9853e+00,  1.4900e+00,\n          5.5686e-01,  7.5948e-01,  5.5474e-01, -3.5650e-01, -3.1621e+00,\n         -3.5959e+00,  9.3313e-02, -2.6987e+00, -2.1884e+00,  1.2104e+00,\n          8.7510e-01,  9.2594e-01,  2.4218e+00,  5.1301e+00, -2.7112e+00,\n          1.7965e+00,  3.7812e+00,  1.1353e+00,  2.1910e+00,  2.1278e+00,\n         -3.9336e-01,  6.9684e+00,  1.4971e+00,  3.3343e+00, -1.7414e+00,\n         -2.0950e+00, -2.8024e+00, -1.9632e+00, -2.8748e+00, -1.5099e+00,\n         -7.4104e-02,  1.3317e+00,  1.4586e+00, -1.6930e+00, -4.8226e-01,\n          1.5557e-01,  2.0725e+00, -6.6463e+00, -7.3844e-01, -3.9859e+00,\n         -2.3786e+00, -1.9579e+00, -3.2142e+00, -1.2175e+00,  1.6109e+00,\n         -1.6936e+00, -2.5023e-01,  1.9520e+00,  2.8960e+00, -2.2722e+00,\n         -3.3260e+00, -3.3712e+00, -1.1211e+00, -1.2867e+00, -1.2047e+00,\n          1.4102e+00,  5.5120e+00,  2.2788e+00, -7.1537e-01, -1.6314e+00,\n         -4.0394e+00, -1.3336e+00, -3.8703e+00, -1.8800e+00, -4.0646e+00,\n         -4.0285e+00, -6.1846e+00, -3.1248e+00, -1.1049e+00, -1.7534e+00,\n         -1.1517e+00, -3.1593e+00, -4.2687e+00, -1.2400e+00, -3.3298e+00,\n         -3.6405e-01, -2.1628e+00, -1.7277e-01, -2.5208e+00, -4.4201e+00,\n         -3.0223e+00, -1.7547e+00, -4.6722e-01, -4.0561e+00, -2.2782e+00,\n         -3.5059e+00, -2.4427e+00, -2.5434e+00, -4.4181e+00, -4.2020e+00,\n         -3.6763e+00, -1.7004e+00, -1.9970e+00, -2.8418e+00, -3.1769e+00,\n          4.3481e+00, -2.8812e+00,  3.3600e+00,  5.5104e-02,  9.5356e-01,\n         -3.8969e+00, -6.2496e-01, -2.1571e+00,  5.8635e-01, -1.1987e+00,\n         -2.1360e+00, -8.2393e-01,  1.1649e+00,  2.0561e-01, -3.0886e+00,\n         -3.5501e+00, -4.3474e+00, -4.6899e+00, -3.4663e+00, -1.0302e+00,\n         -2.3022e+00, -7.2823e-01, -1.6859e+00, -4.6620e+00, -1.1482e+00,\n          2.7125e-01, -1.2924e+00,  3.2062e+00, -1.3226e-01,  9.9160e-01,\n         -3.5134e+00, -2.6296e+00,  8.7732e-02,  9.9284e-01, -1.7033e+00,\n         -4.5980e-01,  5.1761e+00,  9.8227e-01,  2.3876e-01, -1.7728e+00,\n         -8.8844e-01,  1.0034e+00, -3.2367e+00, -8.2115e-01,  4.7448e+00,\n         -3.1384e+00, -3.3244e+00,  1.7884e+00, -7.7009e-01,  3.3258e+00,\n         -2.5429e+00, -9.3809e-01, -9.3613e-01, -8.3585e-01,  3.5260e+00,\n          6.0046e+00,  2.5217e+00,  2.0442e+00,  1.9997e+00, -1.0160e+00,\n         -1.2087e+00,  3.0629e+00,  5.0289e+00,  1.5543e+00,  6.0118e+00,\n          1.1436e+00,  4.0334e-01, -1.7323e+00, -1.3988e+00, -3.1212e+00,\n          2.1204e+00, -2.4763e+00, -3.7161e-01, -1.3006e+00,  3.5386e+00,\n          1.6616e+00,  2.7194e-02,  1.0849e+00,  5.6680e+00,  4.3152e-01,\n         -2.2684e+00,  3.6886e+00,  1.3740e+00,  5.4440e-01, -1.5913e+00,\n         -2.7671e+00,  1.3968e+01,  2.4993e+00, -4.0820e+00, -3.0500e+00,\n          6.1745e+00, -2.1779e+00, -4.5237e-01,  7.9290e+00, -2.7333e+00,\n         -1.8879e+00,  5.2791e+00, -2.4143e+00,  3.9654e+00,  1.6333e+01,\n          2.3812e+00, -1.6990e+00, -1.9914e+00, -8.7469e-01,  2.9397e+00,\n          2.8012e+00,  8.1935e-01,  1.5101e+00,  5.4596e+00, -2.7617e+00,\n          2.9663e+00, -1.4820e-01,  1.5666e+00, -1.8598e+00,  1.3108e+00,\n         -2.9560e+00, -2.5783e+00, -5.0982e+00, -6.6950e-01, -1.3698e+00,\n         -1.0736e+00,  2.7823e+00,  1.3959e+00,  6.3052e+00,  1.3966e+00,\n          5.3384e+00,  1.6515e+00,  2.7523e+00, -9.6901e-01,  6.3715e+00,\n         -3.7731e+00, -3.8492e+00,  1.8670e+00, -1.9608e-01,  2.4439e+00,\n          1.9442e+00,  1.6739e+00,  2.0396e+00, -5.2894e+00,  5.1944e+00,\n          1.1183e+00, -5.2408e-01,  4.1741e+00,  1.3451e+00, -7.0957e+00,\n         -5.0263e+00, -4.8917e+00,  6.3763e+00,  6.6537e-01,  7.4734e+00,\n          3.7522e+00,  5.1481e+00,  5.4042e-01,  1.0460e+00,  2.6684e-01,\n         -7.8196e-01, -1.8095e+00, -3.1066e+00,  9.5755e-01,  5.3636e+00,\n         -2.0626e+00, -3.5112e+00,  1.0549e-01,  5.2150e+00, -6.4570e-01,\n          1.5750e+00,  8.7670e-01, -2.2784e+00, -1.8051e-01, -1.1253e+00,\n         -2.8818e-01, -2.0966e+00, -6.9075e-01, -2.1039e+00,  4.4197e+00,\n         -1.0966e+00,  5.2351e+00,  2.1633e+00,  2.2072e+00,  7.4444e+00,\n          4.0554e+00,  3.2325e+00, -3.0197e+00, -5.2628e+00,  5.6647e+00,\n         -1.1286e+00,  3.0956e+00,  1.1010e+00, -1.4893e+00, -2.6978e+00,\n         -2.2318e+00,  7.9628e+00,  1.0666e+00, -3.2215e+00,  3.5388e+00,\n         -1.6675e+00, -3.2155e+00,  3.0883e+00,  2.3411e+00, -8.3568e-01,\n          1.2040e+00,  1.4780e+00,  6.8942e+00,  9.9502e-01, -6.6757e+00,\n          7.7041e+00,  1.4132e+00,  3.0254e+00, -1.4457e+00,  1.9095e+00,\n         -3.5945e+00,  5.1872e+00,  6.2971e+00, -6.7135e+00,  1.5388e-01,\n         -3.7467e+00,  4.4449e+00, -3.0676e+00,  2.7152e+00,  3.4472e+00,\n         -3.4401e+00,  1.2081e+00,  1.2058e+00,  4.2934e-01,  7.6181e-01,\n         -9.4881e-01,  9.3601e-01, -3.5856e+00,  1.9920e+00,  2.2819e+00,\n         -2.4341e+00,  5.4618e+00,  5.1780e+00, -1.8937e+00,  1.7914e-01,\n          5.4936e+00, -4.1560e-01, -6.9367e-01, -1.5934e+00,  4.3691e+00,\n          1.2821e+00,  6.0301e+00,  7.1624e+00,  1.7979e+00, -1.8148e+00,\n          2.8916e-01,  2.6977e+00, -1.3472e-01, -5.3700e-01, -5.0443e+00,\n          2.9804e+00,  5.7885e-01, -3.5140e+00,  5.4094e+00,  4.0014e+00,\n          3.3655e-01,  1.5400e-02,  1.0315e-01,  5.7886e+00, -2.5609e+00,\n         -3.0454e+00,  4.7566e+00, -2.8893e+00, -2.5591e+00,  7.6338e-01,\n          4.2309e+00, -1.1678e+00, -2.9736e+00,  6.1202e+00,  1.6433e+00,\n          1.9179e+00,  4.1401e+00,  4.4686e+00, -4.0413e+00, -4.4417e-01,\n          1.1993e+01,  8.0792e-01, -7.1774e-01,  6.1999e+00,  4.8063e+00,\n         -4.6399e-01,  2.7030e+00, -3.1632e-01, -4.1276e+00,  2.6275e+00,\n          2.6024e+00, -1.2322e-02,  1.0349e+00,  1.8301e+00, -1.6714e+00,\n         -8.0595e-01, -4.0964e+00,  5.6565e-01, -1.1496e+00,  1.3658e+00,\n         -2.4933e+00,  1.2333e+00, -7.4452e-01, -1.0189e+00, -2.2784e+00,\n         -1.2301e+00, -2.2413e+00, -1.4158e+00,  6.7495e-02, -2.1458e+00,\n          1.7802e-01, -3.3074e-02,  2.8818e+00,  1.9175e+00,  2.0630e+00,\n         -5.1974e+00,  1.1240e+00,  3.5298e+00,  1.3636e+00,  7.4050e+00,\n         -9.8509e-01,  3.6509e-01,  5.1060e+00, -3.2759e+00,  6.5892e+00,\n         -1.6405e+00, -6.3699e-01, -3.2286e+00, -8.1922e-01, -3.0395e+00,\n         -3.1667e+00,  3.2080e-01,  1.2883e+00,  3.3415e+00, -2.7801e+00,\n          8.8593e+00, -6.6961e-01, -3.7736e+00, -4.1294e-01,  8.6248e-03,\n         -1.0756e+00,  2.5836e+00, -3.8978e-01,  3.9726e+00, -3.3111e+00,\n         -1.4560e+00, -3.5598e+00,  2.6001e+00,  3.0726e+00,  3.3223e+00,\n          2.9866e+00,  1.4118e+00,  6.0775e+00, -5.4740e+00,  1.0998e+01,\n          7.6260e+00, -1.1508e+00, -4.2311e+00,  4.1058e-01,  8.6614e-01,\n          4.0993e+00, -3.3503e+00,  4.1065e+00, -2.2211e+00, -7.7014e-01,\n          3.6394e+00, -1.3620e+00, -3.4466e+00, -1.9344e+00,  6.5459e+00,\n         -1.8637e+00, -5.2899e-01, -4.8473e+00, -1.3857e+00, -1.5495e+00,\n         -2.2776e+00,  2.4884e+00, -1.2566e+00, -3.6340e-02, -7.7150e-01,\n         -1.7696e+00,  4.1316e+00, -3.3605e+00,  3.5089e+00,  5.1148e-01,\n          1.7183e+00,  6.6142e+00,  2.2616e+00,  4.7069e+00,  1.5255e+00,\n         -3.8513e+00, -2.5793e+00,  3.2483e+00,  4.1748e+00, -1.2359e+00,\n          1.4434e+00, -8.2042e-01, -3.7442e+00,  1.5012e+00, -3.0551e+00,\n          1.3229e+00,  1.8841e+00, -4.8578e+00,  1.7351e+00, -4.7029e-01,\n          3.0369e+00, -1.8224e+00,  2.8060e+00,  5.4995e+00,  2.2093e+00,\n         -2.7905e-01, -1.4033e+00,  7.0297e+00,  4.2387e+00,  1.3860e+00,\n         -2.4225e+00,  7.6379e-01,  6.9407e+00,  5.8126e+00,  1.2970e+00,\n         -1.4041e+00, -4.7847e-01,  7.2418e-01,  5.0636e+00,  8.0392e-01,\n          1.3459e+00,  8.5427e-01,  1.2314e+01, -4.3296e+00, -1.8062e+00,\n          1.8861e+00, -9.3716e-01,  5.5024e+00, -1.7122e+00, -1.2149e+00,\n          1.8491e+00,  2.5379e+00,  1.6737e+00, -6.9070e-01, -4.8653e+00,\n          5.0614e-01,  1.3230e+00,  8.8095e-01, -3.1494e+00,  8.5273e-02,\n          9.8821e-01, -2.8885e+00, -3.3884e+00,  2.7393e+00,  4.0405e+00,\n         -2.2271e+00,  3.5995e+00, -6.6115e-01,  6.4324e+00, -1.9829e-02,\n         -2.2005e-01,  4.3790e+00, -5.3142e+00,  4.6029e+00, -2.2418e+00,\n          2.1595e+00,  7.5447e-01,  4.7599e+00,  2.7410e+00, -4.5726e+00,\n         -1.3764e+00,  6.8564e+00,  7.4147e+00,  9.2851e+00, -6.3508e-01,\n          4.6164e+00, -4.3402e-01,  3.2685e+00,  4.9280e-01, -1.8742e-01,\n          7.2741e+00,  1.1436e+00,  1.0083e+00, -2.9209e+00,  4.3646e+00,\n         -7.7576e-01,  1.1522e+00, -2.3868e+00,  3.9370e+00,  2.0375e+00,\n          1.1969e+00,  2.8773e+00,  2.8321e+00, -5.6430e+00,  2.2736e-01,\n          2.4451e+00,  3.3904e+00,  1.9522e+00, -4.1161e+00, -6.5612e-02,\n          5.5615e+00, -7.9817e-01,  7.0878e+00,  4.7674e-01,  3.9737e+00,\n         -2.3239e+00,  6.4490e+00,  1.1974e+00,  2.2744e+00, -2.6941e+00,\n         -4.0099e+00,  4.4872e+00, -2.7090e+00,  3.2153e+00, -9.7171e-01,\n          6.7510e-02, -1.7901e+00, -7.5171e-01,  2.0961e+00, -1.3260e+00,\n          1.6261e+00,  3.5292e+00, -2.2554e+00,  1.6909e+00, -1.3817e+00,\n          3.0757e+00,  2.6861e-01, -4.9100e+00,  1.5868e+00,  2.1616e+00,\n          2.0789e+00, -2.4108e+00,  1.6655e+00,  1.4844e+00,  3.9759e+00,\n         -2.2110e+00,  8.3465e+00,  5.8307e+00,  5.8889e+00, -3.4219e-01,\n         -8.9663e-02,  9.1136e-01,  4.9760e-01,  1.2118e-01, -2.2324e+00,\n          4.8348e+00,  3.6918e+00,  6.2548e+00, -3.2737e-01,  4.3252e-01,\n          1.9183e+00, -8.6782e-01,  1.5953e+00,  8.2022e-01, -3.1119e+00,\n          1.1289e+00, -3.2772e+00, -9.6210e-01, -1.3921e+00, -8.7756e-01,\n          1.2611e+00,  1.3919e+00,  1.4197e+00,  7.2854e-01,  4.0298e+00,\n          3.0122e+00,  4.1272e+00,  7.7524e-01,  1.5916e+00,  1.2217e+00,\n          7.2718e-01,  1.5340e-01, -3.4384e-01, -6.9376e-01,  5.8483e-03,\n          1.5315e+00,  1.4182e+00, -4.5570e-01,  3.1952e-01, -1.2782e+00,\n          1.1317e+00,  4.6062e-01,  1.6560e-01,  6.4386e-01, -1.6058e+00,\n          1.8775e+00,  1.3431e+00,  4.8611e-01,  8.2010e-01,  1.4412e+00,\n          9.4292e-01, -2.0181e-01,  5.0265e+00,  1.2154e+00,  3.7349e-01,\n          1.0191e-01,  1.5122e-01,  1.9612e+00, -4.7754e-01, -1.4100e-01,\n          1.6117e+00,  3.2331e+00,  3.0766e+00, -1.9493e+00,  1.3249e-01,\n          2.0720e+00,  3.4500e+00,  1.0896e+00,  3.6225e+00,  1.2140e+01,\n          1.4718e-02,  2.7413e+00,  4.3023e+00,  3.0687e+00,  1.5032e+00,\n         -4.0404e+00,  2.5521e+00, -2.9340e+00,  8.5300e-01,  2.3818e-01,\n         -6.2326e-02, -2.9228e+00,  7.7025e-01, -1.2147e+00, -2.1204e+00,\n          1.4472e+00,  2.6930e+00, -4.7579e+00,  1.7203e+00, -6.5525e+00,\n          6.5310e-01, -2.0389e+00, -1.5146e+00,  3.5993e+00, -4.7431e+00,\n          2.9822e+00, -4.5280e-01,  1.1612e+00,  3.7262e+00,  1.5197e+00,\n          1.6391e+00,  1.3395e+00,  5.5740e+00, -4.1703e+00, -5.4921e-03]],\n       grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([464])"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax(dim=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [],
   "source": [
    "# get the gradient of the output with respect to the parameters of the model\n",
    "pred[:, 464].backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x7faf2b9195b0>"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 480x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pull the gradients out of the model\n",
    "gradients = dense.get_activations_gradient()\n",
    "\n",
    "\n",
    "# pool the gradients across the channels\n",
    "pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "\n",
    "# get the activations of the last convolutional layer\n",
    "activations = dense.get_activations(img).detach()\n",
    "\n",
    "# weight the channels by corresponding gradients\n",
    "for i in range(512):\n",
    "    activations[:, i, :, :] *= pooled_gradients[i]\n",
    "\n",
    "# average the channels of the activations\n",
    "heatmap = torch.mean(activations, dim=1).squeeze()\n",
    "\n",
    "# relu on top of the heatmap\n",
    "# expression (2) in https://arxiv.org/pdf/1610.02391.pdf\n",
    "heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "# normalize the heatmap\n",
    "heatmap /= torch.max(heatmap)\n",
    "\n",
    "# draw the heatmap\n",
    "plt.matshow(heatmap.squeeze())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dimensions :  (630, 612, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "heatmap = heatmap.detach().numpy()\n",
    "img = cv2.imread('data4/ben/kanser2.jpeg')\n",
    "print('Original Dimensions : ',img.shape)\n",
    "width = int(img.shape[1])\n",
    "height = int(img.shape[0])\n",
    "dim = (width, height)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heatmap = cv2.resize(heatmap,dim)\n",
    "heatmap = np.uint8(255 * heatmap)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "superimposed_img = heatmap * 0.6 + img\n",
    "cv2.imwrite('data4/map.jpg', superimposed_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 4., 6.],\n        [1., 2., 3.]])"
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.Tensor([[2,4,6],[1,2,3]]) ;a"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 4., 6.],\n        [1., 2., 3.]], requires_grad=True)"
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad_()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(324., grad_fn=<SumBackward0>)"
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.pow(3).sum()\n",
    "b"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "outputs": [],
   "source": [
    "b.backward()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 12.,  48., 108.],\n        [  3.,  12.,  27.]])"
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "outputs": [],
   "source": [
    "# Inherit from Function\n",
    "class LinearFunction(torch.autograd.Function):\n",
    "\n",
    "    # Note that both forward and backward are @staticmethods\n",
    "    @staticmethod\n",
    "    # bias is an optional argument\n",
    "    def forward(ctx, input, weight, bias=None):\n",
    "        ctx.save_for_backward(input, weight, bias)\n",
    "\n",
    "        output = input.mm(weight.t())\n",
    "        print('output',output)\n",
    "        print('weight',weight)\n",
    "        if bias is not None:\n",
    "            output += bias.unsqueeze(0).expand_as(output)\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # This is a pattern that is very convenient - at the top of backward\n",
    "        # unpack saved_tensors and initialize all gradients w.r.t. inputs to\n",
    "        # None. Thanks to the fact that additional trailing Nones are\n",
    "        # ignored, the return statement is simple even when the function has\n",
    "        # optional inputs.\n",
    "        print('ceyhun')\n",
    "        input, weight, bias = ctx.saved_tensors\n",
    "        print('input degerler', input)\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # These needs_input_grad checks are optional and there only to\n",
    "        # improve efficiency. If you want to make your code simpler, you can\n",
    "        # skip them. Returning gradients for inputs that don't require it is\n",
    "        # not an error.\n",
    "        if ctx.needs_input_grad[0]:\n",
    "            grad_input = grad_output.mm(weight)\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_weight = grad_output.t().mm(input)\n",
    "        if bias is not None and ctx.needs_input_grad[2]:\n",
    "            grad_bias = grad_output.sum(0)\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_features, output_features, bias=True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "        print('beeeeeen')\n",
    "\n",
    "\n",
    "        # nn.Parameter is a special kind of Tensor, that will get\n",
    "        # automatically registered as Module's parameter once it's assigned\n",
    "        # as an attribute. Parameters and buffers need to be registered, or\n",
    "        # they won't appear in .parameters() (doesn't apply to buffers), and\n",
    "        # won't be converted when e.g. .cuda() is called. You can use\n",
    "        # .register_buffer() to register buffers.\n",
    "        # nn.Parameters require gradients by default.\n",
    "        self.weight = nn.Parameter(torch.empty(output_features, input_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(output_features))\n",
    "        else:\n",
    "            # You should always register all possible parameters, but the\n",
    "            # optional ones can be None if you want.\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        # Not a very smart way to initialize weights\n",
    "        nn.init.uniform_(self.weight, -0.1, 0.1)\n",
    "        if self.bias is not None:\n",
    "            nn.init.uniform_(self.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # See the autograd section for explanation of what happens here.\n",
    "        return LinearFunction.apply(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        # (Optional)Set the extra information about this module. You can test\n",
    "        # it by printing an object of this class.\n",
    "        return 'input_features={}, output_features={}, bias={}'.format(\n",
    "            self.input_features, self.output_features, self.bias is not None\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "outputs": [],
   "source": [
    "linear = LinearFunction.apply"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True), tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True))\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1013, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1013, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "output tensor([[ 0.4846,  1.4473, -4.1337],\n",
      "        [-0.9581, -2.0418,  3.3497],\n",
      "        [-0.1014, -2.8248,  0.1715],\n",
      "        [-3.2757, -5.3128,  8.4274],\n",
      "        [-3.0620, -2.3936,  1.6456]], dtype=torch.float64)\n",
      "weight tensor([[ 0.7992,  1.9522, -1.0440,  0.1046,  0.5579],\n",
      "        [ 0.8127,  1.9026,  0.4062,  1.4999,  1.8420],\n",
      "        [-2.3560, -2.1554, -0.4122,  0.2831, -0.2668]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "ceyhun\n",
      "input degerler tensor([[ 0.7336,  0.9637,  1.4246,  0.0660, -0.9012],\n",
      "        [-0.9247, -0.2991, -0.6211,  0.4048, -0.5842],\n",
      "        [ 0.5243, -0.3624, -0.8068, -0.0610, -1.1630],\n",
      "        [-2.0225, -1.4737, -1.2248, -0.1445, -0.0820],\n",
      "        [ 0.3359, -1.4358,  0.7074, -0.7958,  0.5274]], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import gradcheck\n",
    "\n",
    "# gradcheck takes a tuple of tensors as input, check if your gradient\n",
    "# evaluated with these tensors are close enough to numerical\n",
    "# approximations and returns True if they all verify this condition.\n",
    "input = (torch.randn(5,5,dtype=torch.double,requires_grad=True), torch.randn(3,5,dtype=torch.double,requires_grad=True))\n",
    "print(input)\n",
    "test = gradcheck(linear, input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Thread(Thread-27, initial)>\n",
      "<Thread(Thread-28, initial)>\n",
      "<Thread(Thread-29, initial)>\n",
      "<Thread(Thread-30, initial)>\n",
      "<Thread(Thread-31, initial)>\n",
      "<Thread(Thread-32, initial)>\n",
      "<Thread(Thread-33, initial)>\n",
      "<Thread(Thread-34, initial)>\n",
      "<Thread(Thread-35, initial)>\n",
      "<Thread(Thread-36, initial)>\n"
     ]
    }
   ],
   "source": [
    "# Define a train function to be used in different threads\n",
    "import threading\n",
    "def train_fn():\n",
    "    x = torch.ones(5, 5, requires_grad=True)\n",
    "    # forward\n",
    "    y = (x + 3) * (x + 4) * 0.5\n",
    "    # backward\n",
    "    y.sum().backward()\n",
    "    # potential optimizer update\n",
    "\n",
    "\n",
    "# User write their own threading code to drive the train_fn\n",
    "threads = []\n",
    "for _ in range(10):\n",
    "    p = threading.Thread(target=train_fn, args=())\n",
    "    print(p)\n",
    "    p.start()\n",
    "    threads.append(p)\n",
    "\n",
    "for p in threads:\n",
    "    p.join()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Thread(Thread-27, stopped 123145721466880)>,\n <Thread(Thread-28, stopped 123145738256384)>,\n <Thread(Thread-29, stopped 123145755045888)>,\n <Thread(Thread-30, stopped 123145771835392)>,\n <Thread(Thread-31, stopped 123145788624896)>,\n <Thread(Thread-32, stopped 123145805414400)>,\n <Thread(Thread-33, stopped 123145822203904)>,\n <Thread(Thread-34, stopped 123145838993408)>,\n <Thread(Thread-35, stopped 123145855782912)>,\n <Thread(Thread-36, stopped 123145721466880)>]"
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
